{"data":
    [
        {"id":"MDEwOlJlcG9zaXRvcnkyNDU0NjcyNzQ=","name":"Tokenization ",
        "createdAt":"2020-03-06T16:26:54Z","url":"",
        "description":"To encrypt the sensitive Credit Card data and making it PCI DSS Compliant before storing in Hadoop Data Warehouse,Validating the Luhn's Algorithm Implementation of multi-threading and process queues to achieve prioritizing and fault management without impacting other ﬁles.","isFork":false,
        "languages":[{"name":"HTML","iconifyClass":"logos-html-5"},{"name":"CSS","iconifyClass":"logos-css-3"},{"name":"JavaScript","iconifyClass":"logos-javascript"}]},
        
        {"id":"MDEwOlJlcG9zaXRvcnkyNjkwNzUwMjM=","name":"StreamNow ",
            "createdAt":"2020-06-03T11:53:55Z","url":"",
            "description":"A near real-time acquisition and processing of JSON messages pushed to Kafka Topics by Fraud Monitoring System, and Loading it into Hadoop Data Warehouse (Hive tables). Developed a module to integrate with Barclays Notiﬁcation Engine (BNE) to send real-time notiﬁcation alerts via e-mails for failed batches.","isFork":false,
            "languages":[{"name":"HTML","iconifyClass":"logos-html-5"},{"name":"CSS","iconifyClass":"logos-css-3"},{"name":"JavaScript","iconifyClass":"logos-javascript"}]},
        
        {"id":"MDEwOlJlcG9zaXRvcnkyNzM1MTI1NTE=","name":"Payments Acquisition and Optimization",
            "createdAt":"2020-06-19T14:24:55Z","url":"",
            "description":"A huge project with a massive task of acquiring large, sensitive transactional data from Teradata servers to Hadoop warehouse. Developed an automation routine for data acquisition that drastically boosted the process. Beneﬁtted Business Client - approx. GBP 3.3 trillion p.a.","isFork":false,
            "languages":[{"name":"Python","iconifyClass":"logos-python"}]},

        {"id":"MDEwOlJlcG9zaXRvcnkxNTU5NTE3NTk=","name":"Merchant Fraud Risk Analysis (MFR)",
            "createdAt":"2018-11-03T05:00:56Z","url":"",
            "description":"Data acquisition achieved through automation using a specialized utility project built with selenium and Bash Scripting and Completed the full SDLC for 383 SQL tables in just 3 weeks!","isFork":false,
            "languages":[{"name":"Python","iconifyClass":"logos-python"},{"name":"Jupyter Notebook","iconifyClass":"logos-jupyter"}]},
       
        {"id":"MDEwOlJlcG9zaXRvcnkyMDIxNDc4ODA=","name":"Schema Evolution of Hive External Tables",
            "createdAt":"2019-08-13T13:13:00Z","url":"",
            "description":"Thorough POC of intricacies involved in modifying the schemas of Hive external tables created in Parquet and Delimited Text ﬁle format. Led to saving time and efforts to re-process 10 TB of data across 70  Tables.","isFork":false,
            "languages":[{"name":"Python","iconifyClass":"logos-python"}]},

        {"id":"MDEwOlJlcG9zaXRvcnkxODIxMjk3NTQ=","name":"Cloudera Cluster Health Status Reporting",
            "createdAt":"2019-04-18T17:25:51Z","url":"",
            "description":"Built an Automated Bot to extract the Health of the Cluster and its services like HDFS, Hive, Impala, etc. from Cloudera Home. Send reports at regular intervals via e-mail notiﬁcations.","isFork":false,
            "languages":[{"name":"C#","iconifyClass":"logos-c-sharp"}]}]}